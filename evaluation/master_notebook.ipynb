{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn import metrics, utils, ensemble\n",
    "from joblib import Parallel, delayed\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: INSERT MASTER TABLE FILE-NAME\n",
    "\n",
    "master = pd.read_csv('MASTER_TABLE.csv')\n",
    "\n",
    "req_cols = set(['hosp_id', 'y', 'y_score_fourvar', 'y_score_mcures',\n",
    "                'y_scores_four_lst', 'y_scores_mcures_lst', 'race', 'age', 'sex', 'ethnicity',\n",
    "                'outcome', 'outcome_time', 'admission_date',\n",
    "                'final_time_min'])\n",
    "\n",
    "master = master[req_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "master[\"y_scores_mcures_lst_\"] = master[\"y_scores_mcures_lst\"].apply(literal_eval)\n",
    "master[\"y_scores_mcures_lst_eval1\"] = master[\"y_scores_mcures_lst_\"].apply(lambda L: L[1:]) # Exclude first window\n",
    "\n",
    "master[\"y_scores_four_lst_\"] = master[\"y_scores_four_lst\"].apply(literal_eval)\n",
    "master[\"y_scores_four_lst_eval1\"] = master[\"y_scores_four_lst_\"].apply(lambda L: L[1:]) # Exclude first window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_second_case = (master['final_time_min'] > 2880).values\n",
    "master['in_second_use_case'] = in_second_case\n",
    "\n",
    "secondary_four = [np.mean(x[0:12]) if master['in_second_use_case'][i] else np.nan for i,x in enumerate(master['y_scores_four_lst_'].values)]\n",
    "mcures_four = [np.mean(x[0:12]) if master['in_second_use_case'][i] else np.nan for i,x in enumerate(master['y_scores_mcures_lst_'].values)]\n",
    "\n",
    "master['secondary_four'] = secondary_four\n",
    "master['secondary_mcures'] = mcures_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secondary use case y value \n",
    "\n",
    "master['y_secondary'] = (1 - np.isnan(master['outcome_time'])).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_allowed_categories(observed, allowed):\n",
    "    observed = set(observed)\n",
    "    allowed = set(allowed)\n",
    "    intersection = observed.intersection(allowed)\n",
    "    \n",
    "    if observed != intersection:\n",
    "        non_allowed = observed-allowed\n",
    "        print(\"observed non-allowed categories: {}\".format(', '.join([str(i) for i in non_allowed])))\n",
    "        return(False)\n",
    "    else:\n",
    "        return(True)\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def unpack_lists(col_as_list):\n",
    "    unpacked_list = []\n",
    "    for i in col_as_list:\n",
    "        #print(i)\n",
    "        if type(i)==list:\n",
    "            i = ','.join([str(j) for j in i])\n",
    "        i = replace_all(i, {'[': '', ']':''})\n",
    "        i = i.strip().split(',')\n",
    "        i = [float(j.strip()) for j in i]\n",
    "        unpacked_list.append(i)\n",
    "    \n",
    "    return(unpacked_list)\n",
    "\n",
    "def check_list_lengths(lol1, lol2):\n",
    "    #lol = list of lists\n",
    "    for i,j in zip(lol1, lol2):\n",
    "        if len(i)!=len(j):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_reference(lol, ref, list_op=max, eps=None):\n",
    "    for idx, (i,j) in enumerate(zip(lol, ref)):\n",
    "        _val = list_op(i)\n",
    "        if eps is None:\n",
    "            if _val!=j:\n",
    "                print(idx, '\\n', i, '\\n' , j, _val, j in i, '\\n')\n",
    "                return False\n",
    "        elif abs(_val-j)>eps:\n",
    "            print(idx, '\\n', i, '\\n' , j, _val, j in i, '\\n')\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "\n",
    "print(\"All Columns\")\n",
    "req_cols = set(['hosp_id', 'y', 'y_score_fourvar', 'y_score_mcures',\n",
    "                'y_scores_four_lst', 'y_scores_mcures_lst', 'race', 'age', 'sex',\n",
    "                'outcome', 'outcome_time', 'admission_date',\n",
    "                'final_time_min'])\n",
    "\n",
    "r = (req_cols == set(master.columns).intersection(req_cols))\n",
    "print(\"\\tHave all required columns? {}\".format(r))\n",
    "\n",
    "\n",
    "print(\"Age\")\n",
    "r = (master[\"age\"].dtype == int)\n",
    "print(\"\\tIs integer? {}\".format(r))\n",
    "r = (master[\"age\"].min()>=18)\n",
    "print(\"\\tMin>=18? {}\".format(r))\n",
    "r = (master[\"age\"].max()<=90)\n",
    "print(\"\\tMax<=90? {}\".format(r))\n",
    "\n",
    "print(\"Encounters w/ Outcome\")\n",
    "_ = master[master[\"y\"]==1]\n",
    "r = ( (_[\"final_time_min\"] - _[\"outcome_time\"]).sum() == 0)\n",
    "print(\"\\tfinal_time equals outcome_time? {}\".format(r))\n",
    "allowed_outcomes = ['HHFNC', 'MV', 'mortality', 'IV']\n",
    "r = check_allowed_categories(_[\"outcome\"].unique(), allowed_outcomes)\n",
    "print(\"\\tOnly use allowed outcomes? {}\".format(r))\n",
    "\n",
    "print(\"Race\")\n",
    "allowed_races = [\"African American\", \"American Indian or Alaska Native\", \"Asian\", \"Caucasian\", \"Native Hawaiian and Other Pacific Islander\", \"Other\", \"Patient Refused\", \"Unknown\", np.nan]\n",
    "r = check_allowed_categories(master[\"race\"].unique(), allowed_races)\n",
    "print(\"\\tOnly use allowed race categories? {}\".format(r))\n",
    "\n",
    "print(\"Sex\")\n",
    "allowed_sexes = [\"F\", \"M\"]\n",
    "r = check_allowed_categories(master[\"sex\"].unique(), allowed_sexes)\n",
    "print(\"\\tOnly use allowed sex categories? {}\".format(r))\n",
    "\n",
    "print(\"Ethnicity\")\n",
    "allowed_ethnicities = [\"Hispanic or Latino\", \"Non-Hispanic or Latino\", \"Patient Refused\", \"Unknown\"]\n",
    "# r= check_allowed_categories(master[\"ethnicity\"].unique(), allowed_ethnicities)\n",
    "#print(\"\\tOnly use allowed ethnicity categories? {}\".format(r))\n",
    "print(\"\\tUncomment above to do check.\")\n",
    "\n",
    "list_cns = [\"y_scores_four_lst\", \"y_scores_mcures_lst\", \"y_scores_mcures_lst_\", \"y_scores_mcures_lst_eval1\", \"y_scores_four_lst_\", \"y_scores_four_lst_eval1\"]\n",
    "unpacked_list_dict = {cn: unpack_lists(list(master[cn])) for cn in list_cns}\n",
    "nwin = [min(i, 30) for i in list(np.floor(master[\"final_time_min\"]/(4*60)))]\n",
    "\n",
    "print(\"Lists\")\n",
    "r = check_list_lengths(unpacked_list_dict[\"y_scores_four_lst\"], unpacked_list_dict[\"y_scores_mcures_lst\"])\n",
    "print(\"\\tScore lists the same length? {}\".format(r))\n",
    "r = check_list_lengths(unpacked_list_dict[\"y_scores_four_lst_eval1\"], unpacked_list_dict[\"y_scores_mcures_lst_eval1\"])\n",
    "print(\"\\tScore (eval1) lists the same length? {}\".format(r))\n",
    "\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_four_lst_eval1\"], list(master[\"y_score_fourvar\"]), eps=1E-10)\n",
    "print(\"\\tIs fourvar (eval1) max is max of list? {}\".format(r))\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_mcures_lst_eval1\"], list(master[\"y_score_mcures\"]), eps=1E-10)\n",
    "print(\"\\tIs MCURES (eval1) max is max of list? {}\".format(r))\n",
    "\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_four_lst\"], nwin, list_op=len)\n",
    "print(\"\\tIs len of fourvar equal to expected number of windows? {}\".format(r))\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_four_lst_eval1\"], [i-1 for i in nwin], list_op=len)\n",
    "print(\"\\tIs len of fourvar (eval1) equal to expected number of windows-1? {}\".format(r))\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_mcures_lst\"], nwin, list_op=len)\n",
    "print(\"\\tIs len of MCURES equal to expected number of windows? {}\".format(r))\n",
    "r = check_reference(unpacked_list_dict[\"y_scores_mcures_lst_eval1\"], [i-1 for i in nwin], list_op=len)\n",
    "print(\"\\tIs len of MCURES (eval1) equal to expected number of windows-1? {}\".format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def get_nums(df, string = 'y_score_mcures'): \n",
    "    N, perc, val_roc, all_rocs = get_roc(df['y'], df[string])\n",
    "    all_prs, val_pr = get_pr(df['y'], df[string])\n",
    "    return (N, perc), (all_rocs, val_roc), (all_prs, val_pr)\n",
    "\n",
    "def get_roc(y_true, y_score):\n",
    "    roc_curves, auc_scores = zip(*Parallel(n_jobs=4)(delayed(bootstrap_func_roc)(i, y_true, y_score) for i in range(1000)))\n",
    "    val = metrics.roc_auc_score(y_true, y_score)\n",
    "    return len(y_true), np.mean(y_true), val, auc_scores\n",
    "\n",
    "def bootstrap_func_roc(i, y_true, y_score):\n",
    "    while True:\n",
    "        try:\n",
    "            yte_true_b, yte_pred_b = utils.resample(y_true, y_score, replace=True, random_state=i)\n",
    "            return metrics.roc_curve(yte_true_b, yte_pred_b), metrics.roc_auc_score(yte_true_b, yte_pred_b)\n",
    "        except: \n",
    "            i += 1000\n",
    "\n",
    "def bootstrap_func_calib(i, combine_all_scores, all_ys):\n",
    "    yte_true_b, yte_pred_b = utils.resample(combine_all_scores, all_ys, replace=True, random_state=i)\n",
    "    flat_ys = [item for sublist in yte_pred_b for item in sublist]\n",
    "    flat_scores = [item for sublist in yte_true_b for item in sublist]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(flat_ys, flat_scores, strategy = 'quantile')\n",
    "    return fraction_of_positives, mean_predicted_value \n",
    "\n",
    "            \n",
    "def get_calibs(combine_all_scores, all_ys):\n",
    "    fractions, means = zip(*Parallel(n_jobs=4)(delayed(bootstrap_func_calib)(i, combine_all_scores, all_ys) for i in range(1000)))\n",
    "    return fractions, means\n",
    "    \n",
    "def get_roc_CI(y_true, y_score):\n",
    "    roc_curves, auc_scores = zip(*Parallel(n_jobs=4)(delayed(bootstrap_func_roc)(i, y_true, y_score) for i in range(1000)))\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for fpr, tpr, _ in roc_curves:\n",
    "        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        aucs.append(metrics.auc(fpr, tpr))\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)\n",
    "    return roc_curves, auc_scores, mean_fpr, tprs_lower, tprs_upper\n",
    "\n",
    "def bootstrap_func_pr(i, y_true, y_score):\n",
    "    while True:\n",
    "        try:\n",
    "            yte_true_b, yte_pred_b = utils.resample(y_true, y_score, replace=True, random_state=i)\n",
    "            return (\n",
    "                metrics.precision_recall_curve(yte_true_b, yte_pred_b), \n",
    "                metrics.auc(*metrics.precision_recall_curve(yte_true_b, yte_pred_b)[1::-1])\n",
    "            )\n",
    "        except: \n",
    "            i += 1000\n",
    "\n",
    "def get_pr(y_true, y_score):\n",
    "    curves, scores = zip(*Parallel(n_jobs=4)(delayed(bootstrap_func_pr)(i, y_true, y_score) for i in range(1000)))\n",
    "    val = metrics.auc(*metrics.precision_recall_curve(y_true, y_score)[1::-1])\n",
    "    return scores, val\n",
    "\n",
    "def get_pr_CI(y_true, y_score):\n",
    "    curves, scores = zip(*Parallel(n_jobs=4)(delayed(bootstrap_func_pr)(i, y_true, y_score) for i in range(1000)))\n",
    "    precs = []\n",
    "    mean_rec = np.linspace(0, 1, 101)\n",
    "    for prec, rec, _ in curves:\n",
    "        rec_sorted, prec_sorted = rec[np.argsort(rec)], prec[np.argsort(rec)]\n",
    "        precs.append(np.interp(mean_rec, rec_sorted, prec_sorted))\n",
    "    mean_prec = np.mean(precs, axis=0)\n",
    "    std_prec = np.std(precs, axis=0)\n",
    "    prec_upper = np.minimum(mean_prec + 1.96 * std_prec, 1)\n",
    "    prec_lower = np.maximum(mean_prec - 1.96 * std_prec, 0)\n",
    "    return curves, scores, mean_rec, prec_lower, prec_upper\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.validation import check_consistent_length\n",
    "\n",
    "def calibration_curve_ece(y_true, y_prob, *, normalize=False, n_bins=5,\n",
    "                      strategy='uniform'):\n",
    "    y_true = column_or_1d(y_true)\n",
    "    y_prob = column_or_1d(y_prob)\n",
    "    check_consistent_length(y_true, y_prob)\n",
    "\n",
    "    if normalize:  # Normalize predicted values into interval [0, 1]\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "    elif y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\"y_prob has values outside [0, 1] and normalize is \"\n",
    "                         \"set to False.\")\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    if len(labels) > 2:\n",
    "        raise ValueError(\"Only binary classification is supported. \"\n",
    "                         \"Provided labels %s.\" % labels)\n",
    "    y_true = label_binarize(y_true, classes=labels)[:, 0]\n",
    "\n",
    "    if strategy == 'quantile':  # Determine bin edges by distribution of data\n",
    "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "        bins = np.percentile(y_prob, quantiles * 100)\n",
    "        bins[-1] = bins[-1] + 1e-8\n",
    "    elif strategy == 'uniform':\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid entry to 'strategy' input. Strategy \"\n",
    "                         \"must be either 'quantile' or 'uniform'.\")\n",
    "\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    prob_true = bin_true[nonzero] / bin_total[nonzero]\n",
    "    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n",
    "\n",
    "    return np.sum((np.bincount(binids) / sum(np.bincount(binids))) * np.abs(prob_true - prob_pred))\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "def fsgte(score_fx, df, thresholds=None):\n",
    "    given_thresholds = thresholds\n",
    "    _thresholds = []\n",
    "    score_dict = {}\n",
    "    label_dict = {}\n",
    "    \n",
    "    # for each hosp_id, extract a list of monotonically increasing scores\n",
    "    for _, row in df.iterrows():\n",
    "        _pid = row[\"hosp_id\"]\n",
    "        label_dict[_pid] = float(row[\"y\"])\n",
    "        _scores = row[\"y_scores_mcures_lst_eval1\"]\n",
    "        _thresholds += _scores\n",
    "        i = _scores[0]\n",
    "        monotonic_scores = [i]\n",
    "        if len(_scores) > 1:\n",
    "            for j in _scores[1:]:\n",
    "                if j > i:\n",
    "                    monotonic_scores.append(j)\n",
    "                    i = j\n",
    "        score_dict[_pid] = monotonic_scores\n",
    "    \n",
    "    # list of decision thresholds\n",
    "    thresholds = _thresholds if thresholds is None else thresholds    \n",
    "    thresholds = set(thresholds)\n",
    "    thresholds.update({0,1})\n",
    "    thresholds = sorted(thresholds)\n",
    "    thresholded_score_dict = copy.deepcopy(score_dict)\n",
    "    \n",
    "    # for each hosp_id, extract the y label\n",
    "    label_list = []\n",
    "    for pid, _ in thresholded_score_dict.items():\n",
    "        label_list.append(label_dict[pid])\n",
    "    \n",
    "    # calculate per-threshold calibration ece score\n",
    "    scores_arr = []\n",
    "    for threshold in thresholds:\n",
    "        score_list = []\n",
    "        for pid, scores in thresholded_score_dict.items():\n",
    "            if len(scores) > 1:\n",
    "                _scores = [i for i in scores if i>=threshold]\n",
    "                thresholded_score_dict[pid] = _scores\n",
    "            first_score = thresholded_score_dict[pid][0]\n",
    "            score_list.append(first_score)\n",
    "        scores_arr.append(score_fx(label_list, score_list))\n",
    "        \n",
    "    if given_thresholds is None:\n",
    "        return thresholds, scores_arr\n",
    "    else: \n",
    "        return scores_arr\n",
    "    \n",
    "def bootstrap_ece(i, function, df, thresholds):\n",
    "    while True:\n",
    "        try:\n",
    "            df_resample = df.sample(frac=1, replace=True, random_state=i)\n",
    "            return fsgte(function, df_resample, thresholds)\n",
    "        except: \n",
    "            i += 1000\n",
    "    return\n",
    "\n",
    "def get_ece_CI(df):\n",
    "    thresholds, scores = fsgte(calibration_curve_ece, master)\n",
    "    ece_curves = np.array(Parallel(n_jobs=50)(delayed(bootstrap_ece)(i, calibration_curve_ece, df, thresholds) for i in range(100)))\n",
    "    auece_scores = [metrics.auc(thresholds, x) for x in ece_curves]\n",
    "    eces_med, eces_lower, eces_upper = np.percentile(ece_curves, 50, axis=0), np.percentile(ece_curves, 2.5, axis=0), np.percentile(ece_curves, 97.5, axis=0)\n",
    "    return ece_curves, auece_scores, thresholds, eces_med, eces_lower, eces_upper\n",
    "\n",
    "from sklearn.preprocessing import label_binarize \n",
    "\n",
    "def calibration_curve(y_true, y_prob, *, normalize=False, n_bins=5,\n",
    "                      strategy='uniform'):\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    if normalize:  # Normalize predicted values into interval [0, 1]\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "    elif y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\"y_prob has values outside [0, 1] and normalize is \"\n",
    "                         \"set to False.\")\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    if len(labels) > 2:\n",
    "        raise ValueError(\"Only binary classification is supported. \"\n",
    "                         \"Provided labels %s.\" % labels)\n",
    "    y_true = label_binarize(y_true, classes=labels)[:, 0]\n",
    "\n",
    "    if strategy == 'quantile':  # Determine bin edges by distribution of data\n",
    "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "        bins = np.percentile(y_prob, quantiles * 100)\n",
    "        bins[-1] = bins[-1] + 1e-8\n",
    "    elif strategy == 'uniform':\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid entry to 'strategy' input. Strategy \"\n",
    "                         \"must be either 'quantile' or 'uniform'.\")\n",
    "\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    prob_true = bin_true[nonzero] / bin_total[nonzero]\n",
    "    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n",
    "\n",
    "    return prob_true, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 1 Information\n",
    "\n",
    "y_true, y_score = master['y'], master['y_score_mcures']\n",
    "combine_all_scores = [x for x in master[\"y_scores_mcures_lst_eval1\"]]\n",
    "\n",
    "all_ys = [[y_true[i]] * len(combine_all_scores[i]) for i in range(len(y_true))]\n",
    "\n",
    "flat_ys = [item for sublist in all_ys for item in sublist]\n",
    "flat_scores = [item for sublist in combine_all_scores for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(master['admission_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "\n",
    "months1 = ['3/20', '4/20', '5/20']\n",
    "months2 = ['6/20', '7/20', '8/20']\n",
    "months3 = ['9/20', '10/20', '11/20']\n",
    "months4 = ['12/20', '1/21', '2/21']\n",
    "\n",
    "Ns, percs, aurocs, auprs = [], [], [], []\n",
    "for chunks in [months1, months2, months3, months4]: \n",
    "    df = master[master['admission_date'].isin(chunks)]\n",
    "    (N, perc), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "    Ns.append(N)\n",
    "    percs.append(perc)\n",
    "    aurocs.append((all_rocs, roc))\n",
    "    auprs.append((all_prs, pr))\n",
    "\n",
    "all_months = months1 + months2 + months3 + months4\n",
    "\n",
    "Ns_months, percs_months, aurocs_months, auprs_months = [], [], [], []\n",
    "for chunks in all_months: \n",
    "    df = master[master['admission_date'].isin([chunks])]\n",
    "    \n",
    "    if len(df) > 0 and df['y'].sum() > 0:\n",
    "        (N, perc), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "        Ns_months.append(N)\n",
    "        percs_months.append(perc)\n",
    "        aurocs_months.append((all_rocs, roc))\n",
    "        auprs_months.append((all_prs, pr))\n",
    "    else:\n",
    "        Ns_months.append(len(df))\n",
    "        percs_months.append(0)\n",
    "        aurocs_months.append((0, 0, 0))\n",
    "        auprs_months.append((0,0,0))\n",
    "    \n",
    "figure2_results = {\n",
    "    # Across months\n",
    "    'by_months-Ns': Ns,\n",
    "    'by_months-percs': percs,\n",
    "    'by_months-aurocs': aurocs,\n",
    "    'by_months-auprs': auprs,\n",
    "    'granular-Ns': Ns_months, \n",
    "    'granular-percs': percs_months, \n",
    "    'granular-aurocs': aurocs_months, \n",
    "    'granular-auprs': auprs_months\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_quarter_names = []\n",
    "\n",
    "for chunks in [months1, months2, months3, months4]:\n",
    "    by_quarter_names.append('Quarter:'+str(chunks))\n",
    "    \n",
    "by_month_names = []\n",
    "for month in all_months:\n",
    "    by_month_names.append('Month:'+month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Across Demographic Subgroups\n",
    "by_demog_names = []\n",
    "by_demog_Ns = []\n",
    "by_demog_percs = []\n",
    "by_demog_aurocs = []\n",
    "by_demog_auprs = []\n",
    "\n",
    "# Sex\n",
    "for sex in tqdm(np.unique(master['sex']), desc='Sex'):\n",
    "    df = master[master['sex'] == sex]\n",
    "    N, perc = len(df['y']), np.mean(df['y'])\n",
    "    by_demog_names.append('Sex:'+sex)\n",
    "    by_demog_Ns.append(N)\n",
    "    by_demog_percs.append(perc)\n",
    "\n",
    "    if N > 0 and perc > 0 and perc < 1:\n",
    "        (_, _), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "        by_demog_aurocs.append((all_rocs, roc))\n",
    "        by_demog_auprs.append((all_prs, pr))\n",
    "    else:\n",
    "        by_demog_aurocs.append([[0], 0])\n",
    "        by_demog_auprs.append([[0], 0])\n",
    "\n",
    "\n",
    "# Age\n",
    "age_groups = [[17, 25], [25, 45], [45, 65], [65, 85], [85, 1000]]\n",
    "\n",
    "for age_lower, age_upper in tqdm(age_groups, desc='Age'):\n",
    "    df = master[(age_lower < master['age']) & (master['age'] <= age_upper)]\n",
    "    N, perc = len(df['y']), np.mean(df['y'])\n",
    "    by_demog_names.append('Age:{}-{}'.format(age_lower, age_upper))\n",
    "    by_demog_Ns.append(N)\n",
    "    by_demog_percs.append(perc)\n",
    "\n",
    "    if N > 0 and perc > 0 and perc < 1:\n",
    "        (_, _), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "        by_demog_aurocs.append((all_rocs, roc))\n",
    "        by_demog_auprs.append((all_prs, pr))\n",
    "    else:\n",
    "        by_demog_aurocs.append([[0], 0])\n",
    "        by_demog_auprs.append([[0], 0])\n",
    "# Race\n",
    "master['race_category'] = master['race'].replace(\n",
    "    ['African American', 'Caucasian', np.nan, 'Patient Refused', 'Unknown', 'American Indian or Alaska Native', 'Native Hawaiian and Other Pacific Islander'],\n",
    "    ['Black', 'White', 'Other', 'Other', 'Other', 'Other', 'Other']).fillna('Other')\n",
    "\n",
    "for race in tqdm(np.unique(master['race_category']), desc='Race'): \n",
    "    df = master[master['race_category'] == race]\n",
    "    N, perc = len(df['y']), np.mean(df['y'])\n",
    "    by_demog_names.append('Race:'+race)\n",
    "    by_demog_Ns.append(N)\n",
    "    by_demog_percs.append(perc)\n",
    "\n",
    "    if N > 0 and perc > 0 and perc < 1:\n",
    "        (_, _), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "        by_demog_aurocs.append((all_rocs, roc))\n",
    "        by_demog_auprs.append((all_prs, pr))\n",
    "    else:\n",
    "        by_demog_aurocs.append([[0], 0])\n",
    "        by_demog_auprs.append([[0], 0])\n",
    "# Ethnicity\n",
    "ethnicities = [\n",
    "    'Ethnicity:Hispanic',\n",
    "    'Ethnicity:Non-Hispanic',\n",
    "    'Ethnicity:Unknown',\n",
    "]\n",
    "\n",
    "master['ethnicity_category'] = master['ethnicity'].replace(\n",
    "    ['Hispanic or Latino', 'Non-Hispanic or Latino', 'Unknown', 'Patient Refused'],\n",
    "    [    'Ethnicity:Hispanic','Ethnicity:Non-Hispanic','Ethnicity:Unknown', 'Ethnicity:Unknown']).fillna('Ethnicity:Unknown')\n",
    "\n",
    "\n",
    "for ethnicity in tqdm(np.unique(master['ethnicity_category']), desc='Ethnicity'): \n",
    "    df = master[master['ethnicity_category'] == ethnicity]\n",
    "    N, perc = len(df['y']), np.mean(df['y'])\n",
    "    by_demog_names.append(ethnicity)\n",
    "    by_demog_Ns.append(N)\n",
    "    by_demog_percs.append(perc)\n",
    "\n",
    "    if N > 0 and perc > 0 and perc < 1:\n",
    "        (_, _), (all_rocs, roc), (all_prs, pr) = get_nums(df)\n",
    "        by_demog_aurocs.append((all_rocs, roc))\n",
    "        by_demog_auprs.append((all_prs, pr))\n",
    "    else:\n",
    "        by_demog_aurocs.append([[0], 0])\n",
    "        by_demog_auprs.append([[0], 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_demog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_demog_Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 4\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "def bootstrap_fn(i, df, replace = True, val = 48):\n",
    "    df_Yte_agg = df.sample(frac = 1, replace=replace, random_state=i)\n",
    "    \n",
    "    scores = np.sort(df_Yte_agg['secondary_mcures'])\n",
    "    for s in scores: \n",
    "        curr = df_Yte_agg[df_Yte_agg['secondary_mcures'] <= s]\n",
    "        if 1 - curr['y_secondary'].mean() >= 0.95: \n",
    "            latest = curr\n",
    "    try: \n",
    "        discharged = latest.shape[0] / len(scores)\n",
    "        num = latest.shape[0]\n",
    "        total_days = np.sum((latest['final_time_min'] / (60 * 24)) - (val / 24))\n",
    "\n",
    "    except: \n",
    "        return 0, 0\n",
    "    \n",
    "    return discharged, total_days\n",
    "\n",
    "\n",
    "def get_roc_CI(df, val):\n",
    "    discharged, days = zip(*Parallel(n_jobs=4)(delayed(bootstrap_fn)(i, df, val) for i in range(1000)))\n",
    "    return discharged, days\n",
    "\n",
    "    \n",
    "secondary = master[master['in_second_use_case'] == True]\n",
    "\n",
    "import time \n",
    "\n",
    "now = time.time() \n",
    "\n",
    "discharged, days = get_roc_CI(secondary, 48)\n",
    "\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_results = []\n",
    "D_results.append({\n",
    "    'Institution': 'UM',\n",
    "    \n",
    "    # Figure 1\n",
    "    'y_true': list(master['y']),\n",
    "    'y_score': list(master['y_score_mcures']),\n",
    "    'all_predictions': combine_all_scores,\n",
    "\n",
    "\n",
    "    # Figure 2: Across months\n",
    "    'by_month_names': by_month_names,\n",
    "    'by_month_Ns': Ns_months,\n",
    "    'by_month_percs': percs_months,\n",
    "    'by_month_aurocs': aurocs_months,\n",
    "    'by_month_auprs': auprs_months,\n",
    "    \n",
    "    # Figure 2: Across quarters\n",
    "    'by_quarter_names': by_quarter_names,\n",
    "    'by_quarter_Ns': Ns,\n",
    "    'by_quarter_percs': percs,\n",
    "    'by_quarter_aurocs': aurocs,\n",
    "    'by_quarter_auprs': auprs,\n",
    "    \n",
    "    # Figure 3: Across demographics\n",
    "    'by_demog_names': by_demog_names,\n",
    "    'by_demog_Ns': by_demog_Ns,\n",
    "    'by_demog_percs': by_demog_percs,\n",
    "    'by_demog_aurocs': by_demog_aurocs,\n",
    "    'by_demog_auprs': by_demog_auprs,\n",
    "    \n",
    "    # Figure 4\n",
    "    'days_saved_boostraps': list(days), \n",
    "    'discharged_boostrap': list(discharged),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(D_results)\n",
    "#TODO: CHANGE OUTPUT FILE NAME\n",
    "df_results.to_csv('INSTITUTION.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
